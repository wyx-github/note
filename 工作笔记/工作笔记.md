* awk用法：如果只是显示/etc/passwd的账户和账户对应的shell,而账户与shell之间以逗号分割,而且在所有行添加列名name,shell,在最后一行添加"blue,/bin/nosh"

  ```
  cat /etc/passwd |awk  -F ':'  'BEGIN {print "name,shell"}  {print $1","$7} END {print "blue,/bin/nosh"}'
  j
  name,shell
  root,/bin/bash
  daemon,/bin/sh
  bin,/bin/sh
  sys,/bin/sh
  ....
  blue,/bin/nosh
  ```

  

* 查看systemd 的target下有哪些unit

  ```
  systemctl list-dependencies icfs-osd.target
  ```

* 将某个service加入某个target下

  ```
  systemctl enable icfs-osd-admin.service    #该命令会检查/etc/systemd/system下是否有该service，并且应该会根据icfs-osd-admin.service文件中【install】下的wantedby的值(或【unit】x)（例如：icfs-osd.target)将/etc/systemd/system下的service软链接到/usr/lib/systemd/system下的icfs-osd.target.wantsm
  ```

  

* 证书问题如果自测可先将 nginx 配置改下，方便自测方法：

  ```
  vim /usr/local/nginx/conf/nginx.conf +147行
  注释掉：# ssl_verfy_client on; 后，保存
  shell 执行: nginx -s reload
  ```
  
  
  
* 替换ec库

  ```shell
  scp ./src/.libs/libec*so {ip}:/lib64/icfs/erasure-code
  ```
  
  
  
* 查看池子对象列表

  ```
  rados -p {pool_name} ls
  ```

* 删除池子对象

  ```
  rados -p {pool_name} rm {object_name} --force-full
  ```

  

* 重载daemon

  ```
  systemctl daemon-reload
  ```

* 设置pg nearful ration

  ```
  icfs pg set_nearfull_ratio 0.85
  ```

  

* 清除分区数据

  ```
  dd if=/dev/zero of=/dev/sdb1 bs=8k c
  ```
  
  
  
* 管软部署时提示mon分区失败解决方法

  ```
  查看日志 vim /var/log/icfs-cli.log看到illegal sector size 2048,hw sector size is 4096
  修改部署脚本vim /usr/bin/icfs-admin-disktool搜mkfs，将2048改为4096
  然后将/usr/bin/icfs-admin-disktool复制到每个节点相同路径及/usr/local/ism/Agent/src/Cli/ClusterCli/icsf-admin-disktool：ansible "*" -m copy -a "src=/usr/bin/icfs-admin-disktool dest=/usr/bin/icfs-admin-disktool"及ansible "*" -m copy -a "src=/usr/bin/icfs-admin-disktool dest=/usr/local/ism/Agent/src/Cli/ClusterCli/icsf-admin-disktool"
  ```
  
  
  
* 查看已安装包个数

  ```
  rpm -qa|wc
  ```
  
  
  
* 重启（重置）ipmi BMC

  ```
  ipmitool mc reset cold
  ```

* 查询ipmi信息

  ```
  ipmitool lan print 1
  ```

* 修改IPMI ip

  ```
  ipmitool lan set 1 ipsrc static
  ipmitool lan set 1 ipaddr 172.20.1.1
  ipmitool lan set 1 netmask 255.255.0.0           //必须先设置静态IP，然后再设置掩码。
  ipmitool lan set 1 defgw ipaddr  172.20.0.0
  ```

* 查看网口是否连接

  ```
  ethtool eno1		#查看link detected:yes
  ```

  

* 开启8084端口部署集群，在部署节点上执行

  ```
  sh /usr/local/ism/sysmgt/sbin/modify_nginx add
  ```
  
  
  
* icfs tell osd.* injectargs '--cache_tier_no_io_flush_delay=0.1'	

* 修改osd日志级别

  ```
  icfs tell osd.* injectargs '--debug_osd=1\/5'
  ```

  

*  ./vdbench -f smalltest-1 -o output/smalltest0623-1 -vr &

* 100.7.45.150挂载

  ```
  mount /dev/sdm1 /home/code
  ```
  
  
  
* PG unfound解决方法

  ```
  icfs health detail |grep unfound|grep acting|awk -F " " '{print$2}'  #列出处于unfound的pg号
  icfs pg pg号 mark_unfound_
  ```

  

* 查看osd守护进程配置信息

  ```
  icfs daemon osd.0 config show
  icfs daemon osd.0 config show | grep objectstore
  ```

  

* 升级集群

  ```
  ssh 到100.7.35.114，在路径/root/uranus_V2/isodir/***拷贝升级包
  将升级包分别拷到各个节点，解压并进入，在每个节点上执行./icsf_update.sh
  ```

* 节点IPMI

  |     节点     |     IPMI     |
  | :----------: | :----------: |
  | 100.7.45.122 |  100.7.35.6  |
  | 100.7.42.165 | 100.7.42.115 |
  |              |              |

* ansible all -m shell -a "lsblk"

* sigaddset(SIGINT)是针对在进入信号处理函数期间时阻塞该信号，信号处理函数执行完会获取该信号从而中断

* SA_RESETHAND也是针对信号处理函数的，在执行一次型号处理函数后，会恢复到默认处理

* SA_NODEFER也是针对信号处理函数的，正常内核在信号处理函数执行过程中阻塞下一个信号，sa_flags设置此项时不会阻塞，在信号处理函数执行时收到该信号时继续执行下一个信号处理过程

* epoch的用意: 如果网络分割，别人又发起了选举，现任leader不知道，接收方会发现epoch不对,主要针对选举

* last_commited对应的肯定在此前达成quorum一致的 ； 而uncommitted则是认为没有形成quorum一致的，需要处理，一般是旧leader提出但未批准的议案。

* icfs osd down osd.0    #down调osd.0后，会自动up

* systemctl stop icfs-osd@0   #这样会down调osd.0，且经过特定时间后会变为out状态

* mount时提示写保护，只读错误

  需先将盘符格式化，mkfs.xfs /dev/sdj -f

* 模拟插拔盘

  ```shell
  echo 1 > /sys/block/sdb/device/delete   #拔盘
  for i in /sys/class/scsi_host/host*/scan;do echo "- - -">$i;done   #插盘
  ```
  
  
  
* 模拟ssd

  ```shell
  lsblk -d -o name,rota    #查看rota，1为hdd，0为sdd
  然后echo 0 > /sys/block/sdb/queue/rotational   #将sdb的rotational文件值置为0，
  若要部署文件系统，需后台运行脚本，不断执行echo 0 > /sys/block/sdb/queue/rotational即ssd.sh脚本
  ```
  
  
  
* 删除分区

  ```
  icfs-admin-disktool  --delete_part  --disk /dev/nvme5n1 --parts ALL --json     #删除分区
  icfs-disk zap /dev/nvme5n1    #格式化分区
  ```
  
  
  
* bluestore部署步骤

  * icfs-deploy new inspur01	#生成配置文件及keyring

  * icfs-deploy mon create-initial	#部署mon
  
  * icsf-deploy --overwrite-conf admin inspur01  #将本目录下icfs.conf推送到/etc/icfs/icfs.conf或直接cp过去
  * icfs-deploy disk zap inspur01:/dev/sdb   #格式化分区
  
* icfs
  
* crushmap 导入导出

  ```
  icfs osd getcrushmap -o aa #导出到二进制文件aa中
  crushtool -d aa -o bb	#将二进制文件aa解析成bb可视化文件
  vim bb
  crushtool -c bb -o cc	
  icfs osd setcrushmap -i cc	#导入到crushmap中

 

   

* 为主机增加Gerrit权限

  ```
  将某节点（例如100.7.45.150）/root/.ssh/config文件拷贝到当前节点/root/.ssh路径下即可
  ```

  

* 查看当前可执行文件git版本号（commit号）

  ```
  到usr/bin下执行 icfs-mon -v，最后面括号内的字符串就是
  ```




* 生成makefile文件

  ```
  在软件包根目录下执行./do_autogen.sh -J -L -s gnu++17
  ```

  

* 升级g++

  ```
  编辑devtoolset7.3.1/目录下install_new.sh将boost-devel-xxx.rpm后加--nodeps
  
  保存退出执行./install_new.sh
  
  然后再执行生成makefile，最后到src目录下编译特定组件，如：再src目录下执行make icfs-mon -j4（其中j4代表4线程）
  ```

  

* 设置数据恢复优先级

  `icfs osd set recovery-pri adaptable/rwFirst/recoverFirst`

* 查看数据恢复优先级

  ` icfs-system-recovery --query --recovery-priority`

* 界面设置数据恢复优先级路径

  `管软-》系统设置-》服务设置-》存储服务-》增值服务-》数据恢复QoS`

* 系统升级/降级

```
 将压缩包考入节点某个目录，tar -zxvf xxx.tar.gz
  2) cd xxx/
  3) sh icfs-update.sh #离线升级
  4） 查看icfs版本 icfs -v
  5） 重启服务 systemctl stop icfs.target->systemctl start icfs.target
```




* 查看osd

  `icfs osd tree`

* down掉osd.*

  `systemctl stop icfs-osd@*`

* 设置osd outflag

  `icfs osd unset out` 

* 若节点可连互联网，可启动chronyd服务，并其随系统引导启动

```
 systemctl start chronyd.service
  systemctl enable chronyd.service
```



* 调整时间至特定时间点

  * 只修改日期,不修改时间,输入:
  
    ```
    date -s 2007-08-03  
    hwclock -w   #将当前时间写入主板rtc芯片
    ```
  
  * 只修改时间,输入:
  
  ```
  date -s 14:15:00  
  hwclock -w   #将当前时间写入主板rtc芯片
  ```
  
  
    * 同时修改日期时间,注意要加双引号,日期与时间之间有一空格,输入:
  
  ```
  date -s "2007-08-03 14:15:00"  
  hwclock -w   #将当前时间写入主板rtc芯片
  ```
  
  
    * 向时钟源同步时间
  
  ```
  ntpdate -u {时钟源节点ip}
  ```
  
  
    * 同步互联网时间
  
  ```
  ntpdate 0.asia.pool.ntp.org  
  hwclock -w   #将当前时间写入主板rtc芯片
  ```
  
  
  * 设置系统时区为Asia/ShangHai（CST）
  
  ```
  timedatectl set-timezone Asia/Shanghai
  ```
  
* 查看防火墙状态

  ```
  systemctl status firewalld
  ```

* 停掉防火墙，并关闭

  ```shell
  systemctl stop firewalld.service
  systemctl disable firewalld.servie
  ```

* 无密访问，生成ssh密钥

  ```
  ssh-keygen -t rsa -P '' #使用rsa加密，路径默认
  ssh-copy-id root@NODE3  #可在NODE3的/root/下生成authorized_keys
  或将.ssh/下的id_rsa.pub分别拷贝至各个节点（包括本节点）下并重命名为authorized_keys
  ```

  ------

  ### ceph部署

* 部署集群前准备工作
  * 设置时区为CST，并同步各节点事件
    * 将个节点date -s设置时间差小于1分钟之后，修改各节点/etc/ntp.conf，将server 127.127.1.0改为server 时钟源ip,将/etc/ntp/step-tickers的ip改为时钟源ip
    * 重启ntp，systemctl restart ntpd
  * 关闭防火墙
  * 关闭selinux（/etc/sysconfig/selinux）
  * 设置/etc/hosts域名解析
  * 各节点无密访问
  
* rpm -ivh https://mirrors.tuna.tsinghua.edu.cn/ceph/rpm-15.2.6/el8/noarch/ceph-release-1-1.el8.noarch.rpm

* yum install epel-release  #某些组件的依赖

* yum update && yum install ceph-deploy python-setuptools python2-subprocess32  #后两个是可能需要的依赖

* 

* 类继承关系

  * Monitor：Dispatcher，md_config_obs_t
  * MMonPorbe：Message
  * SimpleMessenger::send_message()：Messenger::send_message()=0

* ceph monitor OP_PROBE消息传递流程

  pthread_create()->Thread::\_entry_func()->Thread::entry_wrapper()->DispatchQueue::DispatchThread::entry()->DispatchQueue::entry()->Messenger::ms_deliver_dispatch()->Monitor::ms_dispatch()->Monitor::_ms_dispatch()->Monitor::dispatch_op()->Monitor::handle_probe()

* ceph monitor OP_GET_COOKIE_FULL/RECENT消息传递流程

  ...同probe->Monitor::dispatch_op()->Monitor::handle_sync()->Monitor::handle_sync_get_cookie()

* Elector/Paxos/PaxosService消息传递流程

  均通过Monitor::dispatch_op()调Monitor::Elector/Paxos/PaxosService->dispatch()

* Monitor state状态：
  * STATE_PROBING
  * STATE_SYNCHRONIZING
  * STATE_ELECTING
  * STATE_LEADER
  * STATE_PEON
  * STATE_SHUTDOWN

* Paxos状态：state
  * STATE_RECOVERING
  * STATE_ACTIVE
  * STATE_UPDATING                          //leader/peon is updating a new value
  * STATE_UPDATING_PREVIOUS      //leader proposing an old value
  * STATE_WRITING                             //leader/peon is writing a new commit
  * STATE_WRITING_PREVIOUS         //leader/peon is writing a new commit from a previous round
  * STATE_REFRESH                             //leader refresh following a commit

* 块存储先创建卷才可以往里写数据，创建卷流程

  管软-》设备管理-》块服务-》逻辑卷-》创建

* encode是将结构memcpy到bufferlist，transaction::put是将bufferlist存入transaction::ops中，等待apply_transaction()将该bufferlist存入db；transaction::erase是将bufferlist存入transaction::ops中，等待apply_transaction()将该bufferlist从db中删除

  ### icfs-mon流程

  * 参数解析，环境检查
  * global_init()
    * global_pre_init()
  * 创建MonitorDBStore(构造函数里将创建的leveldb指针赋给keyvaluedb)，然后将/var/lib/icfs/mon/$cluster-$id/store.db的内容读入到创建的的MonitorDBStore对象中
  * store->open()里会调finisher.start()创建FinisherThread线程，进入线程入口函数finisher_thread_entry()不断轮询context容器，不空则执行回调context的complete，进而执行子类的finish(r)，进而执行不同的回调（创建context类对象回调其实就是把当前类指针传进去，让线程按队列顺序执行context类的finish函数）
  * 若有inject monmap则将其版本添加到store中monmap版本后面
  * 检查store中是否有monmap，有的话读出来，没有的话继续往下执行，在下面的条件判断中建立起monmap
  * 从monmap中读ip地址，读不到则从配置文件中读，并生成初始monmap
  * 创建通信器messenger（里面创建的是AsyncMessenger）并和前面得到的ip地址绑定
  * 通过前面得到的store、通信器msgr及monmap构造消息派发者monitor（所有消息均通过它进行分发），其构造函数里面创建各个paxosService服务（做为paxos和上层应用的中间层）
  * monitor预初始化preinit()
    * 
  * 启动msgr通信器(启动socket线程等)，monitor初始化init()
    * bootstrap()
      * OP_PROBE
      * handle_probe_probe:OP_PROBE_REPLY
      * handle_probe_reply:OP_SYNC
      * handle_sync
      * 
    * pg_stat_tp.start()启动线程池（用以接受消息任务）


### 议案触发流程

* monitor收到处理不同类型的议案消息调用paxos_service[*]->dispatch()

  * PaxosService::propose_pending()

    * paxos->queue_pending_finisher(new C_Commited(this));将PaxosService子类指针传入回调（只是放入容器中，等待被调），等议案commit后调commit_propose（）等来手动执行该paxosService里C_Commited中的finish()，即执行paxosService的_active()；

    * paxos->trigger_propose()，出发议案流程

      * paxos::propose_pending()

        * begin()

          * leader:OP_BEGIN==传给==peon:handle_begin()：OP_ACCEPT==传给==leader：handle_accept()，当收到所有accept后，调commit_start()

            * commit_start()调get_store()->queue_transaction)(...,new C_Commited(this))（作用就是创建context类对象回调，把当前类指针传进去，让线程按队列顺序执行context类的finish函数，其实是交给线程即可自动执行finish函数，和PaxosService的不一样（合适时机手动调））

              

* ------

  往块存储池卷里写数据命令

  ```
  rbd bench pool1/ww --io-size 1M --io-total 5000M --io-pattern seq --io-type write
  ```

  其中pool1是存储池名，ww为卷名，1M是每次IO的大小，5000M是写数据总大小

* 使用ceph自带的rados bench工具进行测试

  ```
  该工具的语法是：rados bench -p <pool_name> <seconds> <write|seq|rand> -b <block size> -t --no-cleanup
  
      pool_name：测试所针对的存储池；
  
      seconds：测试所持续的秒数；
  
      <write|seq|rand>：操作模式，write：写，seq：顺序读；rand：随机读；
  
     -b：block size，即块大小，默认为 4M；
  
     -t：读/写并行数，默认为 16；
  
     --no-cleanup 表示测试完成后不删除测试用数据。在做读测试之前，需要使用该参数来运行一遍写测试来产生测试数据，在全部测试结束后可以运行 rados -p <pool_name>    cleanup 来清理所有测试数据。
  例如：rados bench -p rep_pool_3 3600 write -b 4M/1024 -t 1 --no-cleanup
  ```

  

* 灌装系统步骤

  * 将iso镜像拷到VDI桌面，打开VMware sphere，ip：100.7.42.166：root/0000000

* 修改hostname命令

  ```
  hostnamectl --static set-hostname Wyx  #不加--static会导致主机名（Wyx）中的大写字母（W）变成小写（w）
  ```

* 删除分区

  ```
  icfs-admin-disktool  --delete_part  --disk /dev/nvme5n1 --parts ALL --json 
  ```

  其中nvme5n1是除系统盘（sda）外的其他盘（sdb、sdc...）

* 任意路径下执行count_pg_num.sh（不是sh count_pg_num.sh）可查看各个存储池PG均衡效果

* 存储池扩容时集群会认为是recovery（处于降级degraded）状态，缩容时和osd down 类似导致副本丢失处于degraded

* 查看虚拟IP的路径

  ```
  /etc/ism/ism.con
  ```

* 查看MD5值命令

  ```
  md5sum icfs-3.7.18.30.tar.gz
  ```

* 查看重构进度

  ```
  icfs pg recovery progress
  ```

* monitor消息处理流程

  ```
  https://blog.csdn.net/kakaxi8891/article/details/10935595
  ```

* epoch、pn、v

```
cluster map版本号： epoch
propose num ：pn
poxos version：v
```

* 如何设置终端超时不断开

  ```
  将/etc/profile文件中的TMOUT值改为0
  ```

* 创建带日期的文件名

  ```
  touch wyx_`date +%Y%m%d%H`.log
  ```

  其中+号前必须有空格，+号后不能有空格

### 各类消息及对应的回复消息

#### 1. probe阶段（monitor)

**OP_PROBE**===>handle_probe_probe():**OP_REPLY**===>handle_probe_reply():2.sync_start()/3.start_election()

#### 2.sync同步阶段(monitor)

**OP_GET_COOKIE_RECENT/FULL** ===>handle_sync():handle_sync_get_cookie():**OP_COOKIE**===>handle_sync_cookie():sync_get_next_chunk():**OP_GET_CHUNK**===>handle_sync_get_chunk():**OP_CHUNK/OP_LAST_CHUNK**===>handle_sync_chunk():sync_get_next_chunk()则继续循环/sync_finish()

#### 3.election选举阶段(elector)

**OP_PROPOSE**==>handle_propose():defer():**OP_ACK/OP_STOP_ACK**==>handle_ack():victory():**OP_VICTORY**----->handle_victroy():lose_election()  || ----> handle_ack():win_election():paxos::leader_init():4.collect(0)

#### 4.recovery阶段(paxos)

**OP_COLLECT**==>handle_collect():**OP_LAST** ==>handle_last():**OP_COMMIT** ----->handle_commit():store_state()   ||    ---->handle_last():begin():**OP_BEGIN** ==>handle_begin():**OP_ACCEPT** ==>handle_accept():commit_start():回调commit_finish():**OP_COMMIT** ==>handle_commit():store_state()   ||   ---->handle_last():extend_lease():**OP_LEASE** 并注册超时事件，定时事件==>handle_lease():**OP_LEASE_ACK** 并注册超时事件==>handle_lease_ack():收到所有qurom的ack则取消超时处理

#### 5.议案



### 社区15.2.6ceph选举流程

* Monitor::start_election()====》elector.call_election()====》logic.start()
* 候选人：向所有peers发送OP_PROPOSE
* peers ：handle_propose（）====》logic.receive_propose(）
  * 若我的rank号小于候选人的rank号
    * 如果我给别的候选人回复了ack，那么忽略此候选人的propose；
    * 如果没回复过ack，则我提出申诉，重新提出选举
  * 若收到候选人的rank号小于我的
    * 若没给其他候选人回复过ack，则defer()回复ack
    * 若给其他候选人A回复过ack，但A的rank号大于或等于此次propose候选人的rank号，则defer()回复ack
    * 除了以上情况都不回复，忽略
* 候选人：收到OP_ACK调用handle_ack()
  * 当收到所有peer的ack时，宣布胜出declare_victory()，向所有peers发送OP_VICTORY
  * 当超时时间内还没收到所有人ack，但人数超过一半以上则declare_victory()
  * 当超时时间内没收到候选人胜出消息，则重新发起选举或重启
* peers：handle_victory()====》lose_election()
* 候选人：mon->win_election(）
  * paxos->leader_init()
    * leader：collect(0),提升pn，发送OP_COLLECT
    * peon：handle_collect()，若leader的first_committed大于我的last_committed+1，说明我的版本太低，需要重启进行同步。否则如果leader的pn大于我的pn，则将leader的pn落盘，否则，将我的pn通过last发给leader，同时也将我的first_committed，last_committed发给leader，若我有在数据库里比last_committed更新的版本即存在last_committed+1，那么从数据库中取出传给last
    * leader：handle_last()，若peon的first_committed >我的 last_committed + 1，说明我的版本太低，需要重启进行同步；把peon中比我高的版本都罗盘store_state（）。若我的first_committed > 每个peon的 last_committed + 1，说明peon的版本太低，需要重启进行同步。若我的last_committed大于每个peon的last_committed，则向他们发送OP_COMMIT消息。若peon的pn大于我的pn，则重新发起collect(last->pn)。若peon有更新的版本uncommitted_v，调用begin()自己落盘并让其他peon落盘。若无uncommitted_v，则进入extend_lease()
      * COMMIT：handle_accept():commit_start():回调commit_finish():**OP_COMMIT** ==>handle_commit():store_state()
      * begin()：handle_last():begin():将bufferlist落盘再通知其他peon落盘  **OP_BEGIN** ==>handle_begin():将bufferlist落盘**OP_ACCEPT** ==>handle_accept():commit_start():回调commit_finish():**OP_COMMIT** ==>handle_commit():store_state()
      * extend_lease()
        * 向quorum（所有acked_me）的成员发送OP_LEASE，peon收到OP_LEASE进入active并回复OP_LEASE_ACK，
        * 若超时时间内，没有收到quorum所有成员回复的OP_LEASE_ACK,则重启bootstrap进行选举，否则取消该超时事件
        * 注册定时调用extend_lease()，延长租期

### ceph-deploy源码分析

* ceph-deploy是部署Ceph集群的工具，可以通过SSH方式往远程主机上安装 Ceph 软件包、创建集群、增加监视器、收集（或销毁）密钥、增加 OSD 和元数据服务器、配置管理主机，甚至拆除集群。

* script目录下的ceph-deploy文件是ceph-deploy的入口，安装之后是/usr/bin/ceph-deploy。

  ceph-deploy的\_\_main\_\_ 函数调用cli.py的main函数，该main函数调用_main函数

## osd_stat_t流程分析

* prepare_update		

  * prepare_pg_stats(op)      	#MSG_PGSTATS
    * pending_inc.update_stat(op->get_req()->osd_stat)
      * 将op->get_req()->osd_stat赋给PGMap::Incremental::osd_stat_updates

* PGMonitor::update_from_paxos

  * if(format_version == 0)      PGMap::apply_incremental()
    * 将PGMap::Incremental::osd_stat_updates赋给new_stats
    * 调用PGMap::stat_osd_add(new_stats)
      * osd_sum.add(new_stats)
        * fs_perf_stat.add(new_stats)
  * if(format_version == 1)   PGMap::apply_pgmap_delta(bl)
    * pg_map.update_osd(osd,bl)
      * ::decode(osd_stat,p)    #将bl的迭代器p内容反序列化到osd_stat
      * stat_osd_add(osd_stat)      #再将osd_stat内容添加到osd_sum

  





